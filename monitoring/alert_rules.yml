groups:
  - name: system.rules
    rules:
      - alert: HighCPUUsage
        expr: (100 - (avg by(instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)) > 80
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High CPU usage detected"
          description: "CPU usage is above 80% for more than 5 minutes on {{ $labels.instance }}"

      - alert: HighMemoryUsage
        expr: (node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / node_memory_MemTotal_bytes * 100 > 85
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High memory usage detected"
          description: "Memory usage is above 85% for more than 5 minutes on {{ $labels.instance }}"

      - alert: DiskSpaceUsage
        expr: (node_filesystem_size_bytes{mountpoint="/"} - node_filesystem_free_bytes{mountpoint="/"}) / node_filesystem_size_bytes{mountpoint="/"} * 100 > 85
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High disk usage detected"
          description: "Disk usage is above 85% for more than 5 minutes on {{ $labels.instance }}"

      - alert: SystemLoadHigh
        expr: node_load1 > 4
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High system load detected"
          description: "System load is above 4 for more than 5 minutes on {{ $labels.instance }}"

  - name: application.rules
    rules:
      - alert: ServiceDown
        expr: up == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Service {{ $labels.job }} is down"
          description: "Service {{ $labels.job }} on {{ $labels.instance }} has been down for more than 1 minute"

      - alert: HighResponseTime
        expr: http_request_duration_seconds{quantile="0.95"} > 1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High response time detected"
          description: "95th percentile response time is above 1 second for {{ $labels.job }}"

      - alert: HighErrorRate
        expr: rate(http_requests_total{status=~"5.."}[5m]) / rate(http_requests_total[5m]) > 0.05
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "High error rate detected"
          description: "Error rate is above 5% for more than 5 minutes on {{ $labels.job }}"

      - alert: DatabaseConnectionFailure
        expr: pg_up == 0
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Database connection failed"
          description: "Unable to connect to PostgreSQL database for more than 2 minutes"

      - alert: BackendAPIDown
        expr: up{job="backend-api"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Backend API is down"
          description: "Backend API service has been down for more than 1 minute"

      - alert: FrontendDown
        expr: up{job="frontend-nginx"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Frontend service is down"
          description: "Frontend service has been down for more than 1 minute"

  - name: containers.rules
    rules:
      - alert: ContainerCPUUsage
        expr: (sum by(container_name, instance) (rate(container_cpu_usage_seconds_total[5m]))) * 100 > 80
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Container high CPU usage"
          description: "Container {{ $labels.container_name }} CPU usage is above 80% for more than 5 minutes"

      - alert: ContainerMemoryUsage
        expr: (sum by(container_name, instance) (container_memory_usage_bytes)) / (sum by(container_name, instance) (container_spec_memory_limit_bytes)) * 100 > 85
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Container high memory usage"
          description: "Container {{ $labels.container_name }} memory usage is above 85% for more than 5 minutes"

      - alert: ContainerRestarts
        expr: increase(container_restart_count[1h]) > 5
        for: 0m
        labels:
          severity: warning
        annotations:
          summary: "Container restarting frequently"
          description: "Container {{ $labels.container_name }} has restarted more than 5 times in the last hour"

      - alert: ContainerOOMKilled
        expr: increase(container_oom_killed_total[1h]) > 0
        for: 0m
        labels:
          severity: critical
        annotations:
          summary: "Container killed by OOM"
          description: "Container {{ $labels.container_name }} was killed by OOM (Out of Memory)"

  - name: database.rules
    rules:
      - alert: PostgreSQLDown
        expr: pg_up == 0
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "PostgreSQL is down"
          description: "PostgreSQL database has been down for more than 2 minutes"

      - alert: PostgreSQLTooManyConnections
        expr: pg_stat_activity_count > 80
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "PostgreSQL too many connections"
          description: "PostgreSQL has more than 80 active connections"

      - alert: PostgreSQLSlowQueries
        expr: pg_stat_activity_max_tx_duration > 300
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "PostgreSQL slow queries detected"
          description: "PostgreSQL has queries running for more than 5 minutes"

      - alert: PostgreSQLDeadlocks
        expr: increase(pg_stat_database_deadlocks[1h]) > 5
        for: 0m
        labels:
          severity: warning
        annotations:
          summary: "PostgreSQL deadlocks detected"
          description: "PostgreSQL has more than 5 deadlocks in the last hour"

  - name: security.rules
    rules:
      - alert: TooManyFailedLogins
        expr: increase(failed_login_attempts_total[5m]) > 10
        for: 0m
        labels:
          severity: warning
        annotations:
          summary: "Too many failed login attempts"
          description: "More than 10 failed login attempts in the last 5 minutes"

      - alert: SuspiciousActivity
        expr: rate(http_requests_total{status="404"}[5m]) > 10
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High 404 error rate detected"
          description: "High rate of 404 errors might indicate scanning or attacks"

      - alert: TLSCertificateExpiry
        expr: probe_ssl_earliest_cert_expiry - time() < 7 * 24 * 3600
        for: 0m
        labels:
          severity: warning
        annotations:
          summary: "TLS certificate expiring soon"
          description: "TLS certificate for {{ $labels.instance }} will expire in less than 7 days"

  - name: business.rules
    rules:
      - alert: LowScenarioGenerationRate
        expr: rate(scenario_generation_total[5m]) < 0.1
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Low scenario generation rate"
          description: "Scenario generation rate is below 0.1 per second for more than 10 minutes"

      - alert: HighScenarioValidationFailures
        expr: rate(scenario_validation_failures_total[5m]) / rate(scenario_validation_total[5m]) > 0.1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High scenario validation failure rate"
          description: "Scenario validation failure rate is above 10% for more than 5 minutes"

      - alert: LongProcessingTime
        expr: avg_over_time(processing_time_seconds[5m]) > 30
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Long processing time detected"
          description: "Average processing time is above 30 seconds for more than 5 minutes"

  - name: monitoring.rules
    rules:
      - alert: PrometheusTargetDown
        expr: up{job="prometheus"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Prometheus target is down"
          description: "Prometheus target {{ $labels.instance }} has been down for more than 1 minute"

      - alert: GrafanaDown
        expr: up{job="grafana"} == 0
        for: 1m
        labels:
          severity: warning
        annotations:
          summary: "Grafana is down"
          description: "Grafana has been down for more than 1 minute"

      - alert: AlertmanagerDown
        expr: up{job="alertmanager"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Alertmanager is down"
          description: "Alertmanager has been down for more than 1 minute"

      - alert: MetricsIngestionHigh
        expr: rate(prometheus_tsdb_symbol_table_size_bytes[5m]) > 1000000
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High metrics ingestion rate"
          description: "Prometheus metrics ingestion rate is unusually high"